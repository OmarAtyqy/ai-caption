{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable auto reload of modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will be used to load and preprocess the data, as well as train our own captioning model from scratch. If you want to use the model directly, please refer to the `main.py` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was made to be modular, in such a way that you can run only the section that you want. However, some sections may require the output of the previous section to work properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Loading captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.captions_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the annotation data\n",
    "df = load_raw_captions_data(\"./data/captions/captions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the caption dictionary\n",
    "captions_dic = generate_captions_dic(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print info about the caption dictionary\n",
    "n_images = len(captions_dic)\n",
    "n_captions_per_image = len(next(iter(captions_dic.values())))\n",
    "n_captions = n_images * n_captions_per_image\n",
    "\n",
    "print(f\"Number of images: {n_images}\")\n",
    "print(f\"Number of captions per image: {n_captions_per_image}\")\n",
    "print(f\"Total number of captions: {n_captions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the captions by removing any special characters and converting to lower case\n",
    "captions_dic = clean_captions(captions_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print tha captions for the first image\n",
    "print(f\"Captions for the first image:\")\n",
    "for cap in next(iter(captions_dic.values())):\n",
    "    print(cap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the captions dictionary\n",
    "save_captions_dic(captions_dic, \"./data/captions/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Image feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this section, we will be using mutliple pre-trained models to extract features from the images. The models that we will be using are:\n",
    "<ul>\n",
    "    <li>ResNet50</li>\n",
    "    <li>VGG16</li>\n",
    "    <li>InceptionV3</li>\n",
    "    <li>Xception</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.architectures.xception import *\n",
    "from src.utils.image_utils import *\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if GPU is available and enable memory growth\n",
    "print(\"GPU is\", \"available\" if tf.config.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# height and width and channels to which the images will be resized\n",
    "height = 192\n",
    "width = 192\n",
    "n_channels = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the images for the xcepion model using the xception preprocessor\n",
    "xception_images = load_images_from_folder_parallel(\"./data/images/\", image_size=(height, width), preprocess_input=tf.keras.applications.xception.preprocess_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of images loaded: {len(xception_images)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the feature extractors\n",
    "feature_extractor = XceptionFeatureExtractor(input_shape=(height, width, n_channels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the features from the images\n",
    "# Do it in batches to avoid memory issues\n",
    "batch_size = 256\n",
    "xception_features = []\n",
    "for i in range(0, len(xception_images), batch_size):\n",
    "    print(f\"Extracting features from images ({i}, {i + batch_size}) out of {len(xception_images)}...\")\n",
    "    batch = xception_images[i:i + batch_size]\n",
    "    features = feature_extractor.extract_features(batch)\n",
    "    xception_features.append(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save features into json file\n",
    "save_features(xception_features, \"./data/features/xception_features.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free memory\n",
    "del xception_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Caption preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.captions_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the captions dictionary\n",
    "captions_dic = load_captions_dic(\"./data/captions/processed_captions.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the tokenizer\n",
    "tokenizer = create_tokenizer(captions_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the tokenizer\n",
    "save_tokenizer(tokenizer, \"./data/tokenizer/tokenizer.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.data_utils import *\n",
    "from src.utils.captions_utils import *\n",
    "from src.utils.image_utils import *\n",
    "from src.architectures.lstm import *\n",
    "from src.architectures.xception import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the list of image names\n",
    "image_names = load_image_names(\"./data/images/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the required data\n",
    "captions_dic = load_captions_dic(\"./data/captions/processed_captions.json\")\n",
    "tokenizer = load_tokenizer(\"./data/tokenizer/tokenizer.pkl\")\n",
    "features_dic = load_features_as_dic(\"./data/features/xception_features.npy\", filenames=image_names)\n",
    "max_length = get_max_length(captions_dic)\n",
    "\n",
    "# height and width and channels to which the images will be resized\n",
    "height = 192\n",
    "width = 192\n",
    "n_channels = 3\n",
    "\n",
    "feature_extractor = XceptionFeatureExtractor(input_shape=(height, width, n_channels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train and validation sets\n",
    "data_splitter = DataSplitter(\n",
    "    captions_dic=captions_dic,\n",
    "    features_dic=features_dic,\n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train and validation sets\n",
    "train_data, val_data = data_splitter.split_data(val_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "model = Decoder(\n",
    "    input_shape=feature_extractor.output_shape,\n",
    "    vocab_size=len(tokenizer.word_index) + 1,\n",
    "    max_length=max_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "model.fit(\n",
    "    train_generator=train_data,\n",
    "    val_generator=val_data,\n",
    "    epochs=10,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "model.save(\"./models/lstm_model1.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.data_utils import *\n",
    "from src.utils.captions_utils import *\n",
    "from src.utils.image_utils import *\n",
    "from src.architectures.lstm import *\n",
    "from src.architectures.xception import *\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the xception feature extractor\n",
    "height = 192\n",
    "width = 192\n",
    "n_channels = 3\n",
    "feature_extractor = XceptionFeatureExtractor(input_shape=(height, width, n_channels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the LSTM model\n",
    "model = tf.keras.models.load_model(\"./models/lstm_model1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the feature extractor and the LSTM model\n",
    "# Use the functional API to create the model\n",
    "image_input = feature_extractor.model.input\n",
    "sequence_input = model.input[1]\n",
    "\n",
    "image_features = feature_extractor.model(image_input)\n",
    "\n",
    "caption_output = model([image_features, sequence_input])\n",
    "\n",
    "# create the model\n",
    "model = tf.keras.models.Model(inputs=[image_input, sequence_input], outputs=caption_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 122.03it/s]\n"
     ]
    }
   ],
   "source": [
    "# read the test images\n",
    "test_images = load_images_from_folder(\"./data/test/\", image_size=(height, width), preprocess_input=tf.keras.applications.xception.preprocess_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 5s 5s/step\n"
     ]
    }
   ],
   "source": [
    "# run the model on the test images\n",
    "captions = model.predict([test_images, np.zeros((len(test_images), 78))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# convert the captions to text from scratch\n",
    "tokenizer = load_tokenizer(\"./data/tokenizer/tokenizer.pkl\")\n",
    "captions = convert_captions_to_text(captions, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# print the captions\n",
    "print(captions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Machine Learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
