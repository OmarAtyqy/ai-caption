{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable auto reload of modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will be used to load and preprocess the data, as well as train our own captioning model from scratch. If you want to use the model directly, please refer to the `main.py` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was made to be modular, in such a way that you can run only the section that you want. However, some sections may require the output of the previous section to work properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Loading captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.captions_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the annotation data\n",
    "df = load_raw_captions_data(\"./data/captions/captions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the caption dictionary\n",
    "captions_dic = generate_captions_dic(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print info about the caption dictionary\n",
    "n_images = len(captions_dic)\n",
    "n_captions_per_image = len(next(iter(captions_dic.values())))\n",
    "n_captions = n_images * n_captions_per_image\n",
    "\n",
    "print(f\"Number of images: {n_images}\")\n",
    "print(f\"Number of captions per image: {n_captions_per_image}\")\n",
    "print(f\"Total number of captions: {n_captions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the captions by removing any special characters and converting to lower case\n",
    "captions_dic = clean_captions(captions_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print tha captions for the first image\n",
    "print(f\"Captions for the first image:\")\n",
    "for cap in next(iter(captions_dic.values())):\n",
    "    print(cap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the captions dictionary\n",
    "save_captions_dic(captions_dic, \"./data/captions/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Image feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this section, we will be using mutliple pre-trained models to extract features from the images. The models that we will be using are:\n",
    "<ul>\n",
    "    <li>ResNet50</li>\n",
    "    <li>VGG16</li>\n",
    "    <li>InceptionV3</li>\n",
    "    <li>Xception</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.architectures.xception import *\n",
    "from src.utils.image_utils import *\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if GPU is available and enable memory growth\n",
    "print(\"GPU is\", \"available\" if tf.config.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# height and width and channels to which the images will be resized\n",
    "height = 192\n",
    "width = 192\n",
    "n_channels = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the images for the xcepion model using the xception preprocessor\n",
    "xception_images = load_images_from_folder_parallel(\"./data/images/\", image_size=(height, width), preprocess_input=tf.keras.applications.xception.preprocess_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of images loaded: {len(xception_images)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the feature extractors\n",
    "feature_extractor = XceptionFeatureExtractor(input_shape=(height, width, n_channels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the features from the images\n",
    "# Do it in batches to avoid memory issues\n",
    "batch_size = 256\n",
    "xception_features = []\n",
    "for i in range(0, len(xception_images), batch_size):\n",
    "    print(f\"Extracting features from images ({i}, {i + batch_size}) out of {len(xception_images)}...\")\n",
    "    batch = xception_images[i:i + batch_size]\n",
    "    features = feature_extractor.extract_features(batch)\n",
    "    xception_features.append(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save features into json file\n",
    "save_features(xception_features, \"./data/features/xception_features.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free memory\n",
    "del xception_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Caption preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.captions_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the captions dictionary\n",
    "captions_dic = load_captions_dic(\"./data/captions/processed_captions.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the tokenizer\n",
    "tokenizer = create_tokenizer(captions_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the tokenizer\n",
    "save_tokenizer(tokenizer, \"./data/tokenizer/tokenizer.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.data_utils import *\n",
    "from src.utils.captions_utils import *\n",
    "from src.utils.image_utils import *\n",
    "from src.architectures.lstm import *\n",
    "from src.architectures.xception import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31783/31783 [00:00<00:00, 5863539.21it/s]\n"
     ]
    }
   ],
   "source": [
    "# get the list of image names\n",
    "image_names = load_image_names(\"./data/images/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading captions dictionary...\n",
      "Loading tokenizer...\n",
      "Loading features as a dictionary...\n"
     ]
    }
   ],
   "source": [
    "# load the required data\n",
    "captions_dic = load_captions_dic(\"./data/captions/processed_captions.json\")\n",
    "tokenizer = load_tokenizer(\"./data/tokenizer/tokenizer.pkl\")\n",
    "features_dic = load_features_as_dic(\"./data/features/xception_features.npy\", filenames=image_names)\n",
    "max_length = get_max_length(captions_dic)\n",
    "\n",
    "# height and width and channels to which the images will be resized\n",
    "height = 192\n",
    "width = 192\n",
    "n_channels = 3\n",
    "\n",
    "feature_extractor = XceptionFeatureExtractor(input_shape=(height, width, n_channels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31783/31783 [00:00<00:00, 283446.16it/s]\n"
     ]
    }
   ],
   "source": [
    "# split the data into train and validation sets\n",
    "data_splitter = DataSplitter(\n",
    "    captions_dic=captions_dic,\n",
    "    features_dic=features_dic,\n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train and validation sets\n",
    "train_data, val_data = data_splitter.split_data(val_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "model = Decoder(\n",
    "    input_shape=feature_extractor.output_shape,\n",
    "    vocab_size=len(tokenizer.word_index) + 1,\n",
    "    max_length=max_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 2048)]       0           []                               \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 2048)         0           ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 1024)         2098176     ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 1024)         0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)           [(None, 78)]         0           []                               \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 512)          524800      ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 78, 256)      4681984     ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 512)          0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 78, 256)      0           ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 256)          131328      ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    (None, 256)          525312      ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " add_12 (Add)                   (None, 256)          0           ['dense_2[0][0]',                \n",
      "                                                                  'lstm[0][0]']                   \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 256)          65792       ['add_12[0][0]']                 \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 18289)        4700273     ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 12,727,665\n",
      "Trainable params: 12,727,665\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "446/446 [==============================] - 279s 616ms/step - loss: 4.9542 - val_loss: 4.1081\n",
      "Epoch 2/10\n",
      "446/446 [==============================] - 275s 616ms/step - loss: 3.8804 - val_loss: 3.7796\n",
      "Epoch 3/10\n",
      "446/446 [==============================] - 273s 611ms/step - loss: 3.5923 - val_loss: 3.6426\n",
      "Epoch 4/10\n",
      "446/446 [==============================] - 274s 614ms/step - loss: 3.4215 - val_loss: 3.5671\n",
      "Epoch 5/10\n",
      "446/446 [==============================] - 272s 609ms/step - loss: 3.2954 - val_loss: 3.5241\n",
      "Epoch 6/10\n",
      "446/446 [==============================] - 273s 612ms/step - loss: 3.1949 - val_loss: 3.4946\n",
      "Epoch 7/10\n",
      "446/446 [==============================] - 266s 595ms/step - loss: 3.1103 - val_loss: 3.4867\n",
      "Epoch 8/10\n",
      "446/446 [==============================] - 273s 612ms/step - loss: 3.0393 - val_loss: 3.4789\n",
      "Epoch 9/10\n",
      "446/446 [==============================] - 264s 591ms/step - loss: 2.9768 - val_loss: 3.4906\n",
      "Epoch 10/10\n",
      "446/446 [==============================] - 278s 622ms/step - loss: 2.9230 - val_loss: 3.4903\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "model.fit(\n",
    "    train_generator=train_data,\n",
    "    val_generator=val_data,\n",
    "    epochs=10,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "model.save(\"./models/lstm_model1.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.data_utils import *\n",
    "from src.utils.captions_utils import *\n",
    "from src.utils.image_utils import *\n",
    "from src.architectures.lstm import *\n",
    "from src.architectures.xception import *\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the xception feature extractor\n",
    "height = 192\n",
    "width = 192\n",
    "n_channels = 3\n",
    "feature_extractor = XceptionFeatureExtractor(input_shape=(height, width, n_channels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the LSTM model\n",
    "model = tf.keras.models.load_model(\"./models/lstm_model1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n"
     ]
    }
   ],
   "source": [
    "# load the tokenizer\n",
    "tokenizer = load_tokenizer(\"./data/tokenizer/tokenizer.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the feature extractor and the LSTM model\n",
    "# Use the functional API to create the model\n",
    "image_input = feature_extractor.model.input\n",
    "sequence_input = model.input[1]\n",
    "\n",
    "image_features = feature_extractor.model(image_input)\n",
    "\n",
    "caption_output = model([image_features, sequence_input])\n",
    "\n",
    "# create the model\n",
    "model = tf.keras.models.Model(inputs=[image_input, sequence_input], outputs=caption_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 117.24it/s]\n"
     ]
    }
   ],
   "source": [
    "# read the test images\n",
    "test_images = load_images_from_folder(\"./data/test/\", image_size=(height, width), preprocess_input=tf.keras.applications.xception.preprocess_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:06<00:00, 11.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start sized dog with a dishtowel in its mouth and a white dog is looking at the camera and a white dog is in the background with a white dog in its mouth and a white dog in a blue shirt and a white dog in a blue shirt and a white and white dog in a bathroom with a white dog in its mouth and a white dog in a blue shirt and a white and white dog\n",
      "79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:06<00:00, 12.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start a surfer in a black wetsuit and a man in a black shirt and black shorts are surfing in a lake in the ocean with a man in a black shirt and blue jeans and a man in a black shirt and black pants surfing in the ocean in the ocean with a wave in the background and the sun in the background is in the background and the man is in the water and the man is\n",
      "79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:06<00:00, 11.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start the dog is running in the water with a stick in its mouth and a stick in its mouth is running in the water with a stick in its mouth in the background and a black dog in the water with a stick in its mouth and a black dog in the background with a stick in its mouth and a black dog in the background with a stick in its mouth and a black dog in the\n",
      "79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for image in test_images:\n",
    "    cap = generate_desc(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        image=image,\n",
    "        max_length=get_max_length(captions_dic)\n",
    "    )\n",
    "    print(cap)\n",
    "    print(len(cap.split()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Machine Learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
